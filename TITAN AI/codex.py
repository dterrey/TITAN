# Python script to request data from Codex.
# Uses requests and future libraries.
# Should work both on python 2.7 and python 3.x
# Script version 4
# edit HTTP_USER and HTTP_PASS in __init__
# The HTTP_USER and HTTP_PASS are not your Deloitte credentials, but a user:pass generated by us
# just to use Codex API. If you don't have Codex API credentials you can send us an email.
# Usage example:
# >>> cg.search_custom({0: 'www.google.com'}, selected=[1,2,3], limit=1)
# True
# >>> cg.response
# {u'status': u'OK', u'seconds': 0.008211, u'data': {u'normal': [{u'mime type': u'application/msword', u'sha1': u'4c67c7fee8f414a3cc29a97a13c41fa0f3d65bce', u'descr': u'Composite Document File V2 Document, Little Endian, Os: Windows, Version 10.0, Code page: 1252, Author: Neko, Template: Normal.dotm, Last Saved By: Neko, Revision Number: 2, Name of Creating Application: Microsoft Office Word, Create Time/Date: Tue Aug 15 16:31:00 2017, Last Saved Time/Date: Tue Aug 15 16:31:00 2017, Number of Pages: 1, Number of Words: 0, Number of Characters: 0, Security: 0', u'family': u'cybergate', u'filetype': u'office', u'avclass family': u'generic', u'packer detection': None, u'Compilation timestamp': None, u'sha256': u'2c988eb46d5acee90d32257d26f8232611b61adb42c8cec2efa2a9943a020f46', u'md5': u'9d556b6f44fd8195f616823df1f3840c', u'size': 148992}], u'show': [u'md5', u'sha1', u'sha256']}}
import six
if six.PY2:
    from future import standard_library
    standard_library.install_aliases()
    from builtins import str
    from past.builtins import basestring
    from builtins import object
import requests
import json
import re
try:
    from urllib import urlencode
    from urllib import quote_plus
except ImportError:
    from urllib.parse import urlencode
    from urllib.parse import quote_plus
import logging


class CodexGigasInfo(object):

    def __init__(self, base_url=None):
        if base_url is None:
            base_url = 'https://www.codexgigassys.com/api/v1/'
        self.response = {}
        self.response_headers = {}
        self.error_message = {}
        self.status_code = 0
        self.HTTP_USER = "codex_au"
        self.HTTP_PASS = "8fc2a88eff"
        self.BASE_URL = base_url
        self.BASE_URL_SEARCH = base_url + 'search?'
        self.BASE_URL_SEARCH_COUNT = base_url + 'search_count?'
        self.BASE_URL_SEARCH_EXPORT_CSV = base_url + 'search_export_csv?'

    def _request(self, request_url, post_data=None, files=None, delete=False, put=False):
        # it performs the request to the page and
        # returns the received json
        if post_data is None and delete is False and put is False and files is None:
            requests_func = requests.get
        elif delete is True:
            requests_func = requests.delete
        elif put is True:
            requests_func = requests.put
        else:
            requests_func = requests.post

        r = requests_func(request_url, data=post_data, files=files, auth=(self.HTTP_USER, self.HTTP_PASS), timeout=(36000, 36000))
        self.status_code = r.status_code
        self.response_headers = dict(r.headers)
        if r.status_code != 200:
            self.error_message = r
            if r.headers.get('Content-Type') == "application/json":
                self.response = json.loads(r.text)
            return False
        elif r.headers.get('Content-Type') == "application/json":
            self.response = json.loads(r.text)
            return True
        elif r.headers.get('Content-Type') in ["application/vnd.tcpdump.pcap", "application/zip"]:
            self.response = r.content
            return True
        elif r.headers.get('Content-Type').find('text/') != -1:
            self.response = r.text
            return True
        else:
            self.response = r.content
            return True

    def family_categories(self):
        """Returns a list with the existing
        malware categories that Codex uses (about 34 strings)"""
        return self._request(self.BASE_URL + "family_categories")

    def check_lib(self, q):
        params = {"q": q}
        return self._request(self.BASE_URL + "check_lib?" + urlencode(params))

    def check_imp(self, q):
        params = {"q": q}
        return self._request(self.BASE_URL + "check_imp?" + urlencode(params))

    def get_metadata(self, file_hash):
        params = {"file_hash": file_hash}
        return self._request(self.BASE_URL + 'metadata?' + urlencode(params))

    def post_metadata(self, file_hash, project=None, extras=True):
        params = {"file_hash": file_hash, "project": project, "extras": extras}
        return self._request(self.BASE_URL + 'metadata', params)

    def get_report(self, file_hash, object_id=None, export_csv=False):
        params = {"file_hash": file_hash}
        if object_id is not None:
            params["object_id"] = object_id
        if export_csv:
            params["export_csv"] = export_csv
        return self._request(self.BASE_URL + 'report?' + urlencode(params))

    def get_hash_information(self, hash_p, limit=0, count=False):
        params = {"data": urlencode({"1": hash_p}), "limit": str(limit)}
        if not count:
            return self._request(self.BASE_URL_SEARCH, urlencode(params))
        else:
            return self._request(self.BASE_URL_SEARCH_COUNT, urlencode(params))

    def yara(self, hashes):
        hashes_str = ""
        for x in hashes:
            hashes_str += "file_hash" + quote_plus("[]") + "=" + x + "&"
        hashes_str = hashes_str[:-1]
        return self._request(self.BASE_URL + 'yara', hashes_str)

    def process(self, hashes, sync=False):
        if isinstance(hashes, basestring):
            return self._request(self.BASE_URL + 'process?file_hash=' + hashes + '&sync=' + str(sync))
        elif isinstance(hashes, list):
            hashes_str = ""
            for x in hashes:
                hashes_str += x + '\n'
            return self._request(self.BASE_URL + 'process', urlencode({'file_hash': hashes_str}))
        else:
            raise ValueError("hashes is not a string or a list")

    def get_since_date(self, date, limit=0):
        # date should be received with format YYYY-MM-DD
        pattern = re.compile("^[0-9]{4}-[0-9]{2}-[0-9]{2}$")
        if pattern.match(date):
            params = {"data": urlencode(
                {"147": ">=" + date}), "limit": str(limit),
                'selected[]': '147'}
            return self._request(self.BASE_URL_SEARCH, urlencode(params))
        else:
            self.error_message = "Not a valid string of Date"
            return False

    def create_task(self, task_params):
        return self._request(self.BASE_URL + "task", task_params)

    def task_finished(self, task_id):
        return self._request(self.BASE_URL + "task_finished?task_id=" + str(task_id))

    def get_task(self, task_id):
        return self._request(self.BASE_URL + "task?task_id=" + str(task_id))

    def set_source(self, file_id, source_str):
        return self._request(self.BASE_URL + "source", {'file_hash': file_id, 'source': source_str})

    def get_source(self, file_id):
        return self._request(self.BASE_URL + "source?file_hash=" + str(file_id))

    def send_file_to_process(self, fd, payload={}):
        # send de file in fd file descriptor to upload and process in codex
        url = self.BASE_URL + "file/add"
        files = {"file": fd}
        return self._request(url, payload, files)

    def get_count_dynamic(self):
        url = self.BASE_URL + 'count_dynamic'
        return self._request(url)

    def get_av_count(self):
        return self._request(self.BASE_URL + 'av_count')

    def get_queue_count(self):
        return self._request(self.BASE_URL + 'queue_count')

    def get_queue_tasks(self, queue_name=None):
        if queue_name is not None:
            params = '?queue_name=' + queue_name
        else:
            params = ''
        return self._request(self.BASE_URL + 'queue_tasks' + params)

    def get_queue_tasks_summary(self):
        return self._request(self.BASE_URL + 'queue_tasks_summary')

    def get_completed_similarities(self):
        return self._request(self.BASE_URL + 'completed_similarities')

    def get_completed_tasks(self):
        return self._request(self.BASE_URL + 'completed_tasks')

    def get_cron(self):
        return self._request(self.BASE_URL + 'cron')

    def download(self, file_hashes):
        """
        TLDR: you will probably want to use file_get method instead of download method.
        The download method, given a string of hashes seperated by %A0 (NBSP) will
        return 1 passwored zip file with all the files it found. Inside the zip file
        a readme.txt will indicate which files weren't found on the DB.
        """
        params = {"file_hash": file_hashes}
        return self._request(self.BASE_URL + 'download', params)

    def download_pcap(self, file_hash):
        return self._request(self.BASE_URL + 'download_pcap?file_hash=%s' % file_hash)

    def file_get(self, file_hash, raw=False, force_download=False):
        """
        Given a hash (MD5, SHA1, or SHA256) codex wil return the file.
        If the file could be malware, it will be zipped with 'codex' as password, unless
        parameter raw is True. If force_download is True codex will also add an HTTP 
        Content-Disposition header in the response, you won't need it unless you are 
        returning the results to a browser.
        If you want to download many samples and you have a list of hashes you can
        do something like:
        for hash_tmp in hashes:
            cg.file_get(hash_tmp, raw=True)
            if cg.status_code == 200:
                save_binary(cg.response)
        """
        url = 'file/get?file_hash=' + file_hash
        if raw:
            url += '&raw=true'
        if force_download:
            url += '&force_download=true'
        return self._request(self.BASE_URL + url)

    def export(self, file_hashes):
        params = {"file_hash[]": file_hashes}
        return self._request(self.BASE_URL + 'export', params)

    def last_uploaded(self, limit=10):
        return self._request(self.BASE_URL + 'last_uploaded?n=' + str(limit))

    def av_result(self, hash_str, force=False, force_private_key=False):
        append = ""
        if force:
            append += "&force=true"
        if force_private_key:
            append += "&force_private_key=true"
        return self._request(self.BASE_URL + 'av_result?file_hash=' + str(hash_str) + str(append))

    def av_result_post(self, hash_str):
        return self._request(self.BASE_URL + 'av_result', {'file_hash': str(hash_str)})

    def search_custom(self, data_param, limit=0, count=False, selected=None, export_csv=False, formCollection=None, email=None, repeat=None, avoid_empty_email=None):
        params = (("data", urlencode(data_param)), ("limit", str(limit)))
        if repeat is not None:
            params += (('repeat', repeat),)
        if email is not None:
            params += (('email', email), ('send_email', 'true'))
        if avoid_empty_email is not None:
            params += (('avoid_empty_email', avoid_empty_email),)
        if selected is not None:
            for x in selected:
                params += (('selected[]', x),)
        if formCollection is not None:
            for x in formCollection:
                params += (('formCollection[]', x),)
        if count:
            return self._request(self.BASE_URL_SEARCH_COUNT, params)
        elif export_csv:
            return self._request(self.BASE_URL_SEARCH_EXPORT_CSV, params)
        else:
            return self._request(self.BASE_URL_SEARCH, params)

    def office_screenshots(self, date=None, uses_network=None, dropped_an_exe=None, option_and_or=None, skip_uninteresting=False):
        post_data = {'date': date, 'uses_network': uses_network,
                     'dropped_an_exe': dropped_an_exe, 'option_and_or': option_and_or,
                     'skip_uninteresting': skip_uninteresting}
        return self._request(self.BASE_URL + 'office_screenshots', post_data)

    def screenshots(self, date=None, uses_network=None, dropped_an_exe=None, option_and_or=None, skip_uninteresting=False, collections=None):
        params = (())
        if collections is None:
            collections = ['office', 'rtf']
        for x in collections:
            params += (('collection[]', x),)
        post_data = {'date': date, 'uses_network': uses_network,
                     'dropped_an_exe': dropped_an_exe, 'option_and_or': option_and_or,
                     'skip_uninteresting': skip_uninteresting}
        for key, value in list(post_data.items()):
            params += ((key, str(value)),)
        return self._request(self.BASE_URL + 'screenshots', params)

    def search_av_analysis(self, signature, limit=0, count=False, include_missing_samples=False):
        if include_missing_samples:
            key_number_str = "10100"
        else:
            key_number_str = "10000"
        if isinstance(signature, list) and len(signature) == 1:
            params = {"data": urlencode({key_number_str: signature[0]}), "limit": str(limit)}
        elif isinstance(signature, basestring):
            params = {"data": urlencode({key_number_str: signature}), "limit": str(limit)}
        else:  # list and > 1
            signature = list(set(signature))
            count = 0
            data_param = {}
            for sig in signature:
                if count == 0:
                    data_param[key_number_str] = sig
                else:
                    data_param[key_number_str + "." + str(count)] = sig
                count += 1
            params = {"data": urlencode(data_param), "limit": str(limit)}
        if count:
            return self._request(self.BASE_URL_SEARCH_COUNT, params)
        else:
            return self._request(self.BASE_URL_SEARCH, params)

    def add_host_to_whitelist(self, host):
        return self._request(self.BASE_URL + "cuckoo_whitelist", {'host': host})

    def check_host_whitelist(self, host):
        return self._request(self.BASE_URL + "cuckoo_whitelist?" + urlencode({'host': host}))

    def check_bulk_host_whitelist(self, hosts):
        # hosts is a list of domains or IPs (can be mixed).
        # cg.response will be something like this:
        # {u'status': u'OK', u'hosts': {u'whitelisted': [u'www.google.com'], u'non_whitelisted': [u'google.evil-domain.com']}}
        logging.debug('Checking bulk host whitelisted of %s hosts' % len(hosts))
        params = (())
        for host in hosts:
            params += (('hosts[]', host),)
        return self._request(self.BASE_URL + "cuckoo_bulk_whitelist", params)

    def delete_host_whitelist(self, host):
        return self._request(self.BASE_URL + "cuckoo_whitelist", {'host': host}, delete=True)

    def send_file_to_sandbox(self, file_hash, priority="1"):
        # send the sample with file_hash hash to sandbox,
        # the file must exist in codex
        url = self.BASE_URL + "sandbox_this"
        payload = {"file_hash": str(file_hash), "priority": priority}
        return self._request(url, payload)

    def get_search_schedule(self, sid=None):
        url = self.BASE_URL + 'search_schedule'
        if sid is not None:
            url += '/%s' % sid
        return self._request(url)

    def search_schedule_execute(self, sid):
        url = self.BASE_URL + 'search_schedule_execute/' + str(sid)
        return self._request(url)

    def search_schedule_put(self, sid, key_id, value, value_old):
        url = self.BASE_URL + 'search_schedule/' + str(sid)
        payload = {"search_key_id": key_id, "search_key_value": value, "search_key_value_old": value_old}
        return self._request(url, payload, put=True)

    def search_schedule_email_edit(self, sid, email_str, email_str_old):
        url = self.BASE_URL + 'search_schedule_email_edit/' + str(sid)
        payload = {"email": email_str, "email_old": email_str_old}
        return self._request(url, payload)

    def sandbox_avclass_counter(self):
        url = self.BASE_URL + 'sandbox_avclass_counter'
        return self._request(url)
